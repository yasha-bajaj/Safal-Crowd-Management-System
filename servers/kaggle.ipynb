{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultraytics\n!curl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n!ngrok config add-authtoken 2WqPAb6RElk7pdtvp34ihETJmEo_2q6hniy8qmDmFkHjjNSsB","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForCausalLM \nimport os\nimport flask\nimport torch\nfrom PIL import Image\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nflorence_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large-ft\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, trust_remote_code=True).to(device)\nflorence_processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large-ft\", trust_remote_code=True)\n\nMESSAGES = []\n\ndef get_conn_id():\n    import random\n    return \"\".join(random.choices(\"qwertyuiopasdfghjklzxcvbnm1234567890QWERTYUIOPASDFGHJKLZXCVBNM\", k =20))\n\napp = flask.Flask(\"AXC\")\n\n@app.route(\"/flAccess\", methods=[\"POST\"])\ndef flAccess():\n    con_id = get_conn_id()\n    prompt = dict(flask.request.form)[\"prompt\"]\n    file = flask.files[\"files\"]\n    file.save(f\"input{con_id}.png\")\n    \n    inputs = florence_processor(text=prompt, images=Image.open(f\"input{con_id}.png\"), return_tensors=\"pt\").to(device, torch.float16 if torch.cuda.is_available() else torch.float32)\n    generated_ids = florence_model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        do_sample=False,\n        num_beams=3\n    )\n\n    generated_text = florence_processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n    parsed_answer = florence_processor.post_process_generation(generated_text, task=prompt, image_size=(image.width, image.height))\n\n    try:\n        os.remove(f\"input{con_id}.png\")\n    except:\n        pass\n\n    return parsed_answer\n\n@app.route(\"/alert\")\ndef alerts():\n    if len(MESSAGES) == 0:\n        return \"\", 200\n    return MESSAGES[-1], 200\n\n@app.route(\"/add_alert\", methods=['POST'])\ndef add_alerts():\n    MESSAGES.append(dict(flask.request.json)[\"message\"])\n    return \"\",200\n\n@app.route(\"/clear_alerts\")\ndef clear_alerts():\n    MESSAGES = []\n    return \"\", 200\nimport subprocess\nngrok_proc = subprocess.Popen(\"ngrok http --url=apparent-stirred-bluegill.ngrok-free.app 5000\".split())\napp.run(port=5000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T15:41:31.892256Z","iopub.execute_input":"2025-02-18T15:41:31.893127Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\nFlorence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"name":"stdout","text":" * Serving Flask app 'AXC'\n * Debug mode: off\n","output_type":"stream"}],"execution_count":null}]}